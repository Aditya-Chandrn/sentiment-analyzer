{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKNo9RTBj1hA"
   },
   "source": [
    "#Speech Emotion Recognition with MLP Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AztUA74EerMQ"
   },
   "source": [
    "#Dataset\n",
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) \n",
    "\n",
    "---\n",
    "Audio-only files\n",
    "\n",
    "Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each):\n",
    "\n",
    "Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. \n",
    "Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012.\n",
    "\n",
    "Total=2452\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "Toronto emotional speech set (TESS)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
    "\n",
    "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix0uuwXsXIQ6"
   },
   "source": [
    "# Mount google drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "tjUtB3Elrudq",
    "outputId": "75735104-51f9-4cb2-e890-fcac5f1e3db5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrH14Ttlo2EL"
   },
   "source": [
    "# Install following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "spdyH1iXo5lB",
    "outputId": "82aad428-c6f8-4a79-e26e-6023ba37673e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: soundfile in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ananya\\miniconda3\\lib\\site-packages (1.25.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.0.post7)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.2.11)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (0.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from librosa) (1.0.6)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.41.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from pooch>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile numpy sklearn pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "JdmYeo1-fXtz",
    "outputId": "b5202521-fdac-438a-8596-8bd2fe312e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqlY-_LUpD3l"
   },
   "source": [
    "# Make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fyYSbcJjy7i"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QufSKr1ksJc"
   },
   "source": [
    "Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "\n",
    "* mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "* chroma: Pertains to the 12 different pitch classes\n",
    "* mel: Mel Spectrogram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwB6D_vLpOoR"
   },
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    X, sample_rate = librosa.load(os.path.join(file_name), res_type='kaiser_fast')\n",
    "    if chroma:\n",
    "        stft=np.abs(librosa.stft(X))\n",
    "    result=np.array([])\n",
    "    if mfcc:\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xKzPyVcjvyT"
   },
   "source": [
    "Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS & TESS dataset, and a list to hold all 8 emotions- neutral,calm,happy,sad,angry,fearful,disgust,surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgwcKBQzpdgM"
   },
   "outputs": [],
   "source": [
    "# Emotions in the RAVDESS & TESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "# Emotions to observe\n",
    "observed_emotions=['neutral','calm','happy','sad','angry','fearful', 'disgust','surprised']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VjDiYuC3G-D"
   },
   "source": [
    "# Load the data and extract features for each sound file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2axyBFDqI1A"
   },
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob('/features/Actor_*/*.wav'):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, train_size= 0.75,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    base_directory = 'C:\\\\Users\\\\ananya\\\\Speech-Emotion-Recognition-using-ML-and-DL\\\\features'\n",
    "    \n",
    "    # Define a list of emotions you want to consider\n",
    "    observed_emotions = ['emotion1', 'emotion2', ...]  # Modify with your desired emotions\n",
    "\n",
    "    for actor_folder in glob.glob(os.path.join(base_directory, 'Actor_*')):\n",
    "        for file in glob.glob(os.path.join(actor_folder, '*.wav')):\n",
    "            file_name = os.path.basename(file)\n",
    "            emotion = emotions[file_name.split(\"-\")[2]]\n",
    "\n",
    "            # Check if the emotion is in the list of observed emotions\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "\n",
    "            feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, train_size=0.75, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    base_directory = r'C:\\Users\\ananya\\Speech-Emotion-Recognition-using-ML-and-DL\\features'\n",
    "    \n",
    "    # Define a list of emotions you want to consider\n",
    "    observed_emotions = ['emotion1', 'emotion2', ...]  # Modify with your desired emotions\n",
    "\n",
    "    for actor_folder in glob.glob(os.path.join(base_directory, 'Actor_*')):\n",
    "        for file in glob.glob(os.path.join(actor_folder, '*.wav')):\n",
    "            file_name = os.path.basename(file)\n",
    "            emotion = emotions[file_name.split(\"-\")[2]]\n",
    "\n",
    "            # Check if the emotion is in the list of observed emotions\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "\n",
    "            feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, train_size=0.75, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []  # Initialize empty lists to store features (x) and labels (y).\n",
    "\n",
    "    base_directory = 'C:\\\\Users\\\\ananya\\\\Speech-Emotion-Recognition-using-ML-and-DL\\\\TESS_Toronto_emotional_speech_set_data'  # Update with your directory path\n",
    "\n",
    "    # Iterate through all folders in the base directory.\n",
    "    for folder in glob.glob(os.path.join(base_directory, '*')):\n",
    "        if not os.path.isdir(folder):\n",
    "            continue  # Skip non-directory entries\n",
    "\n",
    "        # Extract emotion from the folder name (e.g., 'OAF_angry' -> 'angry').\n",
    "        emotion = os.path.basename(folder).split('_')[1]\n",
    "\n",
    "        # Check if the extracted emotion is in the list of observed emotions.\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "\n",
    "        # Iterate through audio files within the folder.\n",
    "        for file in glob.glob(os.path.join(folder, '*.wav')):\n",
    "            # Extract audio features (e.g., MFCC, chroma, mel) from the audio file.\n",
    "            feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "\n",
    "            # Append the extracted feature and its corresponding emotion label to the lists.\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "\n",
    "    # Split the data into training and testing sets using train_test_split.\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, train_size=0.75, random_state=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKhGTlqpqOAr"
   },
   "source": [
    "# Split the Dataset\n",
    "Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a92bN5vAqRb2"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "import time\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []  # Initialize empty lists to store features (x) and labels (y).\n",
    "\n",
    "    base_directory = 'C:\\\\Users\\\\ananya\\\\Speech-Emotion-Recognition-using-ML-and-DL\\\\TESS_Toronto_emotional_speech_set_data'  # Update with your directory path\n",
    "\n",
    "    # Iterate through all folders in the base directory.\n",
    "    for folder in glob.glob(os.path.join(base_directory, '*')):\n",
    "        if not os.path.isdir(folder):\n",
    "            continue  # Skip non-directory entries\n",
    "\n",
    "        # Extract emotion from the folder name (e.g., 'OAF_angry' -> 'angry').\n",
    "        folder_name = os.path.basename(folder)\n",
    "\n",
    "        # Split the folder name by underscores and check if it contains at least two elements.\n",
    "        folder_parts = folder_name.split('_')\n",
    "        if len(folder_parts) >= 2:\n",
    "            emotion = folder_parts[1]\n",
    "        else:\n",
    "            # Handle the case where the folder name doesn't contain an underscore or has insufficient parts.\n",
    "            # For example, you can set a default emotion or take other appropriate action.\n",
    "            emotion = \"unknown\"  # Set to \"unknown\" if no emotion is found.\n",
    "\n",
    "        # Check if the extracted emotion is in the list of observed emotions.\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "\n",
    "        # Iterate through audio files within the folder.\n",
    "        for file in glob.glob(os.path.join(folder, '*.wav')):\n",
    "            # Extract audio features (e.g., MFCC, chroma, mel) from the audio file.\n",
    "            feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "\n",
    "            # Append the extracted feature and its corresponding emotion label to the lists.\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "\n",
    "    # Split the data into training and testing sets using train_test_split.\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, train_size=0.75, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    # Load the audio file\n",
    "    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "\n",
    "    # Initialize an empty feature array\n",
    "    result = np.array([])\n",
    "\n",
    "    # Extract features as specified\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_MXFh4lwqYaC",
    "outputId": "eb020e32-a754-4510-97b1-8aaddfba58de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3939, 1313)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmwRPZe7qey5"
   },
   "source": [
    "# Number of features extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46eeiQbYqT7p"
   },
   "source": [
    "#Observe the shape of the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resampy in c:\\users\\ananya\\miniconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from resampy) (1.25.1)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from resampy) (0.58.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\ananya\\miniconda3\\lib\\site-packages (from numba>=0.53->resampy) (0.41.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "H3C6czeDqgxX",
    "outputId": "8625bb60-0a8b-4f66-9b50-88a536ea8b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 153\n"
     ]
    }
   ],
   "source": [
    "# Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEvGnx7r3W7f"
   },
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvggD94ZqmAY"
   },
   "outputs": [],
   "source": [
    "# Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cG4YLLXGqpAb"
   },
   "source": [
    "#Fit/train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "9OfJEVsDqrbY",
    "outputId": "b488b813-c7c9-475e-e527-9428af91ae5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDC7AkAt21EF"
   },
   "source": [
    "# Predict the accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2Kzw2Emqt1p"
   },
   "source": [
    "Let’s predict the values for the test set. This gives us y_pred (the predicted emotions for the features in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNVNDVnBqw1z"
   },
   "outputs": [],
   "source": [
    "# Predict for the test set\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njU6Gs6jqzkg"
   },
   "source": [
    "To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_sDEv4K1q1VA",
    "outputId": "b701c4ab-54b0-4173-ebe5-da659c5dd44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.56%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGdh6fh02hFc"
   },
   "source": [
    "#classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "2xreI8SSq6Rc",
    "outputId": "e7a791de-bbc6-47d7-fdbe-ab80a0f8e53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.99      1.00      0.99        92\n",
      "     disgust       1.00      1.00      1.00       107\n",
      "       happy       1.00      0.99      0.99        91\n",
      "     neutral       0.99      1.00      1.00       114\n",
      "         sad       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           1.00       450\n",
      "   macro avg       1.00      0.99      0.99       450\n",
      "weighted avg       1.00      1.00      1.00       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZZZBmK32ksS"
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "Y2IEjtiTtl-f",
    "outputId": "f5c2e30f-b101-4728-9547-f68b4774cbdb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   0   0   0   0]\n",
      " [  0 107   0   0   0]\n",
      " [  1   0  90   0   0]\n",
      " [  0   0   0 114   0]\n",
      " [  0   0   0   1  45]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test,y_pred)\n",
    "print (matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio features from the test audio file\n",
    "test_feature = extract_feature('C:\\\\Users\\\\ananya\\\\Speech-Emotion-Recognition-using-ML-and-DL\\\\features\\\\Actor_05\\\\03-01-02-01-01-01-05.wav', mfcc=True, chroma=True, mel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the same scaler used during training\u001b[39;00m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 5\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_features\u001b[49m)  \u001b[38;5;66;03m# Assuming you have the training features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_feature_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_features)  # Assuming you have the training features\n",
    "test_feature_normalized = scaler.transform(test_feature.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbipGM_Xnfyb"
   },
   "source": [
    "#Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FinalSER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
